{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBvbXBfbna9N"
   },
   "source": [
    "# EECS16B HW10: Orthonormalization for Model Order Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wjhmGTAna9N"
   },
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from scipy.linalg import solve_triangular\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "data_u = np.load(\"data_u.npy\")\n",
    "data_x = np.load(\"data_x.npy\")\n",
    "# load validation data\n",
    "data_u_valid = np.load(\"data_u_valid.npy\")\n",
    "data_x_valid = np.load(\"data_x_valid.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YniQTxmD-i-"
   },
   "source": [
    "We are given input data $u$ and $x$ that satisfies\n",
    "$$ x[i+1] = b_1 u[i] + b_2 y[i-1] + \\cdots + b_N y[i-(M-1)] + w[i] $$\n",
    "Where $w[i]$ is a bounded noise term in our observations $x[i+1]$.\n",
    "\n",
    "Our goal is to find unknown parameters $\\vec{p}_M= \\begin{bmatrix}b_1, b_2, \\cdots, b_M\\end{bmatrix}^\\top$, as well as the optimal value of $M$.\n",
    "\n",
    "Note that we have two sets of data: [data_u, data_x] and [data_u_valid, data_x_valid], both sampled for 4000 timesteps. The former will be used to fit the parameters, and the latter will be used for evaluating the parameters. We call them the training dataset and validation dataset respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sTQmZvOna9O"
   },
   "source": [
    "In order to solve for the parameters of the FIR filter, we form the given data $u$ and $x$ into a matrix equation. $D_M\\vec{p}_M\\approx\\vec{s}_M$\n",
    "\n",
    "The function `FIR_form_matrix_equation` returns $D_M$ and $\\vec{s}_M$ for a value of $M$ and $M_\\mathrm{max}$ where $M_\\mathrm{max}$ is the maximum order we are considering for our system. The reason for this variable is to insure uniformity of the length of the column vectors we have in our $D_M$ matrices for different model orders we are evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dRP3bwpkna9O",
    "outputId": "5db68f2b-c0ca-4bfe-f0ea-a07f1091669f"
   },
   "outputs": [],
   "source": [
    "def FIR_form_matrix_equation(data_u, data_x, M, Mmax):\n",
    "    # Returns D matrix and s matrix for a certain model order M\n",
    "    L = len(data_x)\n",
    "    D = np.zeros((L-Mmax,M))\n",
    "    s = data_x[M:L-Mmax + M]\n",
    "    for i in range(L-Mmax):\n",
    "        D[i,:] = data_u[i:i+M][::-1] \n",
    "    return D, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, Mmax = 10, 500\n",
    "D_10, s_10 = FIR_form_matrix_equation(data_u, data_x, M, Mmax)\n",
    "print('The shapes of the data matrix and measurement data for N=10 and Nmax=500')\n",
    "print(D_10.shape, s_10.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCLSsJ0Sna9P"
   },
   "source": [
    "The function `LS` is a wrapper function for the built-in `numpy` least squares function to get the solution for $Dp\\approx s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rE8Do7glna9P",
    "outputId": "cbd5b6fc-d476-4564-e8cd-69636a466920"
   },
   "outputs": [],
   "source": [
    "def LS(D, s):\n",
    "    # compute least square solution P of Dp = s\n",
    "    p,_,_,_= np.linalg.lstsq(D, s,rcond=None)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_10 = LS(D_10, s_10)\n",
    "print('Estimated parameters for N=10, Nmax=500\\n',p_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJsrvN1qna9P"
   },
   "source": [
    "The function `calculate_LS_error` computes the error between the predicted output $\\vec{s}_{pred}$ and $\\vec{s}$ in a average square error sense from some parameters $\\vec{p}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZUshBRBCna9P",
    "outputId": "5c6f8910-fdb9-4ede-8ea6-a8b491fdf7bf"
   },
   "outputs": [],
   "source": [
    "def calculate_LS_error(D, s, p):\n",
    "    # compute least squares error\n",
    "    s_pred = D @ p\n",
    "    error = (np.mean((s - s_pred)**2))**0.5\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = calculate_LS_error(D_10, s_10, p_10)\n",
    "print('Error for model order M=10:',error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxmTMjzena9Q"
   },
   "source": [
    "In order to find the optimal model order and the model parameters, we loop over computing the least squares solution to $D_M\\vec{p}_M \\approx \\vec{s}_M$ for possible orders $M=1,2, ... ,500$ with $M_\\mathrm{max} = 500$.\n",
    "\n",
    "**Comment on the optimal order $M$ from the error plot generated from running the code below.**\n",
    "\n",
    "Note that after some value of $M$ you will notice that while the training error decreases, the validation error gradually increases. \n",
    "This is because when the number of parameters $M$ is too big, the model has unnecessary parameters in it as compared to the real pattern that needs to be learned. While in an ideal world, you'd hope that those additional parameters would be fit to zero, that is not what happens with least-squares. They get small nonzero values because of the idiosyncracies of the noise. We call this phenomena overfitting. When overfitting occurs, one may observe validation error increasing because the parameters starts to fit information in the training data that the validation data does not have.\n",
    "We can avoid overfit by selecting the simplest order where the validation error is smallest.\n",
    "\n",
    "The optimal case can be found by picking the $M$ right before the validation error increases, as this will be the $M$ for which we have minimum validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "hbDGNYH4na9Q",
    "outputId": "488ae3b3-79d1-43be-cd33-aea8323f4992"
   },
   "outputs": [],
   "source": [
    "# helper function for plotting errors\n",
    "def plot_errors(Ms, train_errors, validation_errors):\n",
    "    plt.plot(Ms, train_errors, label=\"train\")\n",
    "    plt.plot(Ms, validation_errors, label=\"validation\")\n",
    "    plt.xlabel(\"Order\")\n",
    "    plt.ylabel(\"Mean Square Error\")\n",
    "    plt.legend()\n",
    "    plt.gcf().set_dpi(100)\n",
    "    plt.show()\n",
    "\n",
    "# Straightforward attempt to solve the problem.\n",
    "def FIR_solve(data_u, data_x, data_u_valid, data_x_valid, Ms, Mmax):\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_errors = []\n",
    "    validation_errors = []\n",
    "    \n",
    "    # Initialize data matrices. No reason to pay this cost repeatedly\n",
    "    D, s = FIR_form_matrix_equation(data_u, data_x, Mmax, Mmax)\n",
    "    D_valid, s_valid = FIR_form_matrix_equation(data_u_valid, data_x_valid, Mmax, Mmax)\n",
    "\n",
    "    for M in Ms:\n",
    "        \n",
    "        # Solve the least squares problem just for M parameters by only using the first M columns. \n",
    "        p = LS(D[:,:M], s)\n",
    "\n",
    "        # calculate training error\n",
    "        train_error = calculate_LS_error(D[:,:M], s, p)\n",
    "        train_errors.append(train_error)\n",
    "\n",
    "        # calculate validation error \n",
    "        validation_error = calculate_LS_error(D_valid[:,:M], s_valid, p)\n",
    "        validation_errors.append(validation_error)\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f\"FIR_solve: total time {t1-t0:.3f}sec\")\n",
    "\n",
    "    plot_errors(Ms, train_errors, validation_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are on a local machine with decent resources, when running this block, change Mmax to 750 or 1000 to see how the run time fares.\n",
    "Mmax = 500\n",
    "Ms = list(range(1,Mmax + 1))\n",
    "FIR_solve(data_u, data_x, data_u_valid, data_x_valid, Ms, Mmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Vufh5A7na9Q"
   },
   "source": [
    "## Part (b)\n",
    "In this part, we will speed up the nested least squares solutions by leveraging the QR decomposition.\n",
    "The QR decomposition of a matrix $D$ is the factorization of $D$ into an orthonormal matrix $Q$ and upper-triangular matrix $R$. $Q$ a matrix that would be output by running Gram-Schmidt on $D$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjlrmCUOna9R"
   },
   "source": [
    "The nested nature of the subspaces and the upper-triangular nature of $R$ lets us reuse a lot of work. In the orthonormal basis provided by the $Q$, the solution to the least-squares problem of order $M+1$ is just the solution of order $M$, with one additional parameter. Furthermore, because of the upper-triangular nature of $R$, we can update the values for the learned parameters in the original basis as well without taking inverses for all $R$ of different orders. This is what you need to figure out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuixBmVyp789"
   },
   "source": [
    "**Complete the function `FIR_solve_fast` to test for order $M=1,2,\\dots,500$.**\n",
    "\n",
    "**Comment on the runtime of this method, comparing the runtime with the least squares case above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill in the missing lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FIR_solve_fast(data_u, data_x, data_u_valid, data_x_valid, Mmax):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    train_errors = []\n",
    "    validation_errors = []\n",
    "    \n",
    "    # Set up the big data matrices as before\n",
    "    X, y = FIR_form_matrix_equation(data_u, data_x, Mmax, Mmax)\n",
    "    X_val, y_val = FIR_form_matrix_equation(data_u_valid, data_x_valid, Mmax, Mmax)\n",
    "    \n",
    "    # Something new: now we precompute the QR decomposition for our data matrix\n",
    "    Q, R = np.linalg.qr(X)\n",
    "    # We also invert the big R matrix exactly once so we have it available. \n",
    "    # Remember what the columns of the inverse mean.\n",
    "    Rinv = np.linalg.inv(R)\n",
    "    \n",
    "    # Now, we can also easily do the projection into the big orthonormal basis\n",
    "    # This gives us the least-squares solution in the orthonormal coordinates\n",
    "    # And \"prefixes\" of this give us the least-squares solutions for lower-order\n",
    "    # models, just in the orthonormal coordinates\n",
    "    proj = Q.T @ y\n",
    "    \n",
    "    # This sets up the saving of the learned parameters in the original basis. \n",
    "    # It is initialized to all zeros since initially, we don't know anything.\n",
    "    # We want to save all of the parameters for each order we have learned along the way.\n",
    "    # This is to use the ones that best fit some purpose.\n",
    "    params = np.zeros((len(y), Mmax))\n",
    "    params[0,0] = Rinv[0, 0]*proj[0]\n",
    "    \n",
    "\n",
    "    # Now we loop through the model orders\n",
    "    for i in range(Mmax-1):\n",
    "        p = None\n",
    "        #### STUDENT CODE START\n",
    "        \n",
    "        # FILLIN: Compute the new parameters for the next model order based on the previous model order\n",
    "        \n",
    "        # FILLIN: Evaluate the p used for evaluating training error and validation error\n",
    "        \n",
    "        #### STUDENT CODE END ###\n",
    "        \n",
    "        train_error = calculate_LS_error(X[:,:i+1], y, p)\n",
    "        train_errors.append(train_error)\n",
    "        validation_error = calculate_LS_error(X_val[:,:i+1], y_val, p)\n",
    "        validation_errors.append(validation_error)\n",
    "    p = params[:Mmax,Mmax-1]\n",
    "    train_error = calculate_LS_error(X[:,:Mmax], y, p)\n",
    "    train_errors.append(train_error)\n",
    "    validation_error = calculate_LS_error(X_val[:,:Mmax], y_val, p)\n",
    "    validation_errors.append(validation_error)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(f\"FIR_solve_fast: total time {t1-t0:.3f}sec\")    \n",
    "\n",
    "    Ms = list(range(1,Mmax+1))\n",
    "    plot_errors(Ms, train_errors, validation_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIR_solve_fast(data_u, data_x, data_u_valid, data_x_valid, 500)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "QR_model_order_sol.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "217de1c702d066b83723445a2e43f58cc6e8153a3a352ce69dbc2a3432919b73"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
