{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b39729",
   "metadata": {},
   "source": [
    "# Applied PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff8f653",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880adc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib.widgets import Slider\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f965da",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c6a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.set_xlim([-10, 10])\n",
    "    ax.set_ylim([-10, 10])\n",
    "    ax.set_zlim([-10, 10])\n",
    "    ax.scatter(data[0,:], data[1,:], data[2,:])\n",
    "\n",
    "def plot_data_with_orig(data, orig_fname):\n",
    "    with open(orig_fname, 'rb') as f:\n",
    "        data_orig = np.load(f)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.set_xlim([-10, 10])\n",
    "    ax.set_ylim([-10, 10])\n",
    "    ax.set_zlim([-10, 10])\n",
    "    ax.scatter(data[0,:], data[1,:], data[2,:])\n",
    "    ax.scatter(data_orig[0,:], data_orig[1,:], data_orig[2,:], color=\"red\", alpha=0.3)\n",
    "\n",
    "def plot_pcs(data, l, plane=True):\n",
    "    assert l == 1 or l == 2\n",
    "    Ul = find_principal_components(data, l)\n",
    "    Ll = find_pc_magnitudes(data, l)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.set_xlim([-10, 10])\n",
    "    ax.set_ylim([-10, 10])\n",
    "    ax.set_zlim([-20, 20])\n",
    "    if plane:\n",
    "        if l == 2:\n",
    "            ax.scatter(data[0,:], data[1,:], data[2,:])\n",
    "            normal = np.cross(Ul[:,0].squeeze(), Ul[:,1].squeeze())\n",
    "            xx, yy = np.meshgrid(range(-10,10), range(-10,10))\n",
    "            z = (-normal[0]*xx - normal[1]*yy)/normal[2]\n",
    "            ax.plot_surface(xx,yy,z,color=\"red\", alpha=0.5)\n",
    "        if l == 1:\n",
    "            ax.scatter(data[0,:], data[1,:], data[2,:])\n",
    "            ax.plot([-20*Ul[0,0], 20*Ul[0,0]], [-12*Ul[1,0], 12*Ul[1,0]], zs=[-12*Ul[2,0], 12*Ul[2,0]], color=\"red\")\n",
    "    else:\n",
    "        ax.scatter(data[0,:], data[1,:], data[2,:], alpha=0.2)\n",
    "        origin = [0,0,0]\n",
    "        for i in range(Ul.shape[1]):\n",
    "            ax.quiver(0,0,0,Ll[i,i]*Ul[0,i],Ll[i,i]*Ul[1,i],Ll[i,i]*Ul[2,i],color=\"red\",linewidth=3)\n",
    "\n",
    "def diagonalize(X):\n",
    "    lamb, U = np.linalg.eigh(X)\n",
    "    idx = lamb.argsort()[::-1]\n",
    "    lamb = lamb[idx]\n",
    "    U = U[:,idx]\n",
    "    L = np.diag(lamb)\n",
    "    assert np.allclose(U @ L @ U.T, X)\n",
    "    return U, L\n",
    "\n",
    "def number_of_kb(data):\n",
    "    return (data.size * data.itemsize)//1000\n",
    "\n",
    "def plot_size_vs_acc(data, ls):\n",
    "    sizes = []\n",
    "    accuracies = []\n",
    "    for l in ls:\n",
    "        sizes.append(number_of_kb(compress_data(data, l)) + (data.shape[0] * l)*data.itemsize//1000)\n",
    "        accuracies.append(measure_accuracy(data, l))\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    ax.set_xlabel(\"Size of data (KB)\")\n",
    "    ax.set_ylabel(\"Reconstruction error\")\n",
    "    ax.plot(sizes, accuracies)\n",
    "    fig2 = plt.figure()\n",
    "    ax2 = fig2.add_subplot()\n",
    "    ax2.set_xlabel(\"Number of Principal Components\")\n",
    "    ax2.set_ylabel(\"Size of data (KB)\")\n",
    "    ax2.plot(ls, sizes)\n",
    "\n",
    "def get_de_meaned(X):\n",
    "    return X - np.mean(X, axis=1)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9de43c",
   "metadata": {},
   "source": [
    "## Applications of PCA\n",
    "\n",
    "In this notebook, we will examine several use cases for PCA. First, write code to find the top $\\ell$ principal components of any matrix $A$.\n",
    "\n",
    "_HINT: Use the following function to diagonalize a matrix:_\n",
    "```py\n",
    "diagonalize(X):\n",
    "'''\n",
    "X: matrix to diagonalize\n",
    "\n",
    "returns: (U, Lambda) where the diagonal components of Lambda are sorted in decreasing order\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e8516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_principal_components(A, l):\n",
    "    '''\n",
    "    A: data matrix\n",
    "    l: number of principal components to pick out\n",
    "    \n",
    "    returns: top l principal components of A\n",
    "    '''\n",
    "    ### YOUR CODE HERE ###\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482382af",
   "metadata": {},
   "source": [
    "### De-Noising Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d17a1e",
   "metadata": {},
   "source": [
    "We can start by loading the data and plotting it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae1660",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_noisy.npy', 'rb') as f:\n",
    "    data_noisy = np.load(f)\n",
    "plot_data(data_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463c569",
   "metadata": {},
   "source": [
    "Suppose we know that only two dimensions of this (three-dimensional) data are actually informative, and the remaining dimension is just noise. Note that we don't know exactly which dimensions are good and which is noisy. We can apply PCA to de-noise the data and select these informative dimensions. Write a function to de-noise the data.\n",
    "\n",
    "_HINT: Note that $A \\begin{bmatrix} \\vec{x}_1 & \\vec{x}_2 & \\cdots & \\vec{x}_n \\end{bmatrix} = \\begin{bmatrix} A \\vec{x}_1 & A \\vec{x}_2 & \\cdots & A \\vec{x}_n \\end{bmatrix}$_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd3f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_noise(data, Ul):\n",
    "    '''\n",
    "    data: the data we want to de-noise\n",
    "    Ul: the top l principal components of the data\n",
    "    \n",
    "    returns: de-noised data\n",
    "    '''\n",
    "    ### YOUR CODE HERE ###\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73d1234",
   "metadata": {},
   "source": [
    "Choose the top $\\ell = 2$ principal components to de-noise the data. Plot it using the `plot_data` function:\n",
    "```py\n",
    "plot_data(data):\n",
    "'''\n",
    "data: the data to plot with a scatter plot\n",
    "\n",
    "returns: a scatter plot of the data\n",
    "'''\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2347cee9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "U2 = find_principal_components(data_noisy, 2)\n",
    "de_noised_data = de_noise(data_noisy, U2)\n",
    "plot_data(de_noised_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475a80f1",
   "metadata": {},
   "source": [
    "The data is now much less noisy! The ability for PCA to remove small amounts of noise like this makes it very valuable in data science applications, including system ID. We can also plot the de-noised data against the original data without the noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b278731",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_with_orig(de_noised_data, 'data_orig.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da81bb7",
   "metadata": {},
   "source": [
    "As we can see, PCA has done quite a good job of recovering the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b1dc99",
   "metadata": {},
   "source": [
    "### Data Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505f117c",
   "metadata": {},
   "source": [
    "We can start by loading the data and determining its dimensionality and size (in bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5139a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_large.npy', 'rb') as f:\n",
    "    data_large = np.load(f)\n",
    "print(\"Data dimensionality:\", data_large.shape[0])\n",
    "print(\"Size of data (in KB):\", number_of_kb(data_large))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41418235",
   "metadata": {},
   "source": [
    "We can also use PCA to compress high-dimensional data. Write a function to compress the data matrix to $\\ell$ dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78908330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_data(data, l):\n",
    "    '''\n",
    "    data: the data matrix\n",
    "    l: the desired dimensionality of the data\n",
    "    \n",
    "    returns: l-dimensional data\n",
    "    '''\n",
    "    ### YOUR CODE HERE ###\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce2ac41",
   "metadata": {},
   "source": [
    "We would like to measure the reconstruction error (i.e. the \"closeness\") of the $\\ell$-dimensional approximation to the original data. Write a function to compute the reconstruction error.\n",
    "\n",
    "_HINT: Recall how we motivated the idea of best rank-$\\ell$ approximation of a matrix. How did we define \"best\"?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdaf660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_accuracy(data, l):\n",
    "    '''\n",
    "    data: the data matrix\n",
    "    l: the dimensionality of the approximation\n",
    "    \n",
    "    returns: accuracy of the l-dimensional approximation\n",
    "    '''\n",
    "    ### YOUR CODE HERE ###\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39dd8ae",
   "metadata": {},
   "source": [
    "We can now compute the size (in bytes) of the $\\ell$-dimensional approximation, for some values of $\\ell$ of our choosing. We can then plot size vs reconstruction error. Note that our data is 20-dimensional, so we need to choose $\\ell \\leq 20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e496a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [2, 3, 5, 10, 15] # Feel free to change this list!\n",
    "plot_size_vs_acc(data_large, ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5863077",
   "metadata": {},
   "source": [
    "As we can see, a larger matrix overall will result in a better approximation. However, we can get a fairly decent approximation even with half the size (e.g. 40 KB). Note that, by using this method, you still have to store the $U_\\ell$ matrix, but this storage cost included in the plot above.\n",
    "\n",
    "The idea is very important when performing matrix multiplications on devices that don't have a lot of memory, e.g. your TI Launchpad. However, note that, when you are performing matrix multiplications in these instances, you'd want to use the following order of operations: $$U_\\ell \\left(U_\\ell^\\top \\vec{x}\\right)$$ where $\\vec{x}$ is the vector you are trying to multiply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84ddcdd",
   "metadata": {},
   "source": [
    "### [OPTIONAL] Directions of Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c56af3",
   "metadata": {},
   "source": [
    "This is an optional section that follows up on question 1.e of the associated discussion worksheet. The goal is to investigate why centering our data is important when we want to associate principal components with directions of maximal spread of our data. Much of the mathematical reasoning behind this relies on some statistics, so it remains optional and out of scope.\n",
    "\n",
    "We start by loading our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13461395",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_var.npy', 'rb') as f:\n",
    "    data_var = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8878242",
   "metadata": {},
   "source": [
    "The task here is to find the directions of the data with largest variance. We can observe the magnitude of the principal components, i.e. the top $\\ell$ eigenvalues of $\\frac{1}{N - 1} A A^\\top$, where $N$ is the number of data points we have. Here, we require that $A$ is zero-mean (i.e. each row of $A$ is zero-mean). We will see why this is the case, by visually comparing the results if we de-mean (i.e. subtract out the mean of all the data points from each data point) and don't de-mean. The ideas behind de-meaning, dividing by $\\frac{1}{N - 1}$, and using principal components to find directions of \"variance\" are rooted in statistics and hence out of the scope of this course.\n",
    "\n",
    "First, write a function to find the top $\\ell$ eigenvalues of $\\frac{1}{N - 1} A A^\\top$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec6b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pc_magnitudes(A, l):\n",
    "    '''\n",
    "    A: data matrix\n",
    "    l: number of principal component magnitudes\n",
    "    \n",
    "    returns: magnitude of top l principal components\n",
    "    '''\n",
    "    ### YOUR CODE HERE ###\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f51054",
   "metadata": {},
   "source": [
    "Note that we don't need to rewrite our code to find principal components, since the eigenvectors of $A A^\\top$ will not change if you multiply by a $\\frac{1}{N - 1}$ scaling factor.\n",
    "\n",
    "Use the following function\n",
    "```py\n",
    "plot_pcs(data, Ul, Ll):\n",
    "'''\n",
    "data: the given data points\n",
    "Ul: the top l PCs\n",
    "Ll: the top l eigenvalues\n",
    "\n",
    "returns: a scatter plot of the data with scaled PC vectors\n",
    "'''\n",
    "```\n",
    "to plot the principal components with their respective magnitudes. First, de-mean the data and plot the principal components. Compare this plot with the plot of the original data.\n",
    "\n",
    "_HINT: Use the following function to de-mean your data:_\n",
    "\n",
    "```py\n",
    "get_de_meaned(X):\n",
    "'''\n",
    "X: data matrix\n",
    "\n",
    "returns: de-meaned data matrix\n",
    "'''\n",
    "```\n",
    "\n",
    "#### De-Meaned Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde20ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 2 # change this to any value in {1, 2}\n",
    "draw_plane = False # change this to True if you just want to visualize directions\n",
    "de_meaned_data = get_de_meaned(data_var)\n",
    "plot_pcs(de_meaned_data, l, plane=draw_plane)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa96326",
   "metadata": {},
   "source": [
    "We see that de-meaning the data is informative here for visualizing the directions of variance (and their relative magnitudes) in the data.\n",
    "\n",
    "#### Original Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7131a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plane = False # change this to True if you just want to visualize directions\n",
    "plot_pcs(data_var, l, plane=draw_plane)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb87331",
   "metadata": {},
   "source": [
    "As you may notice, these arrows do not make much sense in terms of describing the variance (i.e., \"shape\") of the data when plotted. Also, notice how they are offset from the center of the data. De-meaning will allow us to use the principal components to better interpret the variance of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f2f8d6",
   "metadata": {},
   "source": [
    "**Contributors**\n",
    "- Anish Muthali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a04966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
